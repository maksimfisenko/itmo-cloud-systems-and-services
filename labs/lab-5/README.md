# Лабараторная 5 #
## Основное задание ##
В этом задание нужно было настроить мониторинг сервиса поднятого в кубере. Для этого я воспользовался следующими инструментами:
1. minikube - для запуска кластера
2. Nginx - приложение, которые будем монитрорить
3. Prometheus - для сбора метрик
4. Grafana - для визуализации данных  
### Подготовка ###
Я работал на компьютере на Windows, на котором до этого не стояло ничего для работы с кубером. Поэтому перед тем как начать делать задание, нужно было утановить все необходимые инструменты. Пойдём по наростанию сложности установки 
#### minikube ####
Тут всё совсем просто: достаточно зайти на [их сайт](https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download), скачать exe файл и запустить его. И всё! Дальше можно в консоли написать minikube и он будет работать. Даже перезагружать ничего не надо!
#### Kubectl ####
Вот сейчас, когда пишу отчёт, узнал, что оказывается Kubectl обязательно устанавливать до minikube. Но у меня всё установилось и в обратном порядке. Тут также надо было скачать файл с [сайта](https://kubernetes.io/releases/download/#binaries), а вот дальше пришлось вспомнинать как добавлять путь в переменную PATH, а потом перезагружаться, чтобы всё заработало. Но после этих манипуляций всё сразу же заработало.
#### Helm ####
Helm нам потребуется, чтобы подтянуть репозитории с Prometheus и Grafana, и установить их. Как поставить его на Windows я понял не сразу (и уже почти пожелел, что начал делать лабу на нём), но потом я узнал, что оказывается в PowerShell есть свой менеджер пакетов **winget**. И можно установить helm одной командой.
~~~ps1
winget install Helm.Helm
~~~
После этой команды можно продолать работать с helm в той же консоли.
Репозитории Prometheus и Grafana далее можно добавить дальше в консоли 
~~~ps1
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
~~~
И вот теперь можно начинать работу!
### Настраиваем кубер ###
Первым делом запустим minikube
~~~ps1
minikube start
~~~
Запустим Prometheus. Репозиторий уже был скачан и теперь достаточно одной команды helm 
~~~ps1
helm install prometheus prometheus-community/prometheus
~~~
Мониторить будем сервер nginx (потому что его легко установить и понятно как можно проверить, что метрики действительно собираются). Для развёртывания использовался специальный файл *deployment.yml* 
~~~yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
          name: nginx-config
      - name: nginx-exporter
        image: nginx/nginx-prometheus-exporter:latest
        args:
          - "-nginx.scrape-uri=http://127.0.0.1:80/stub_status"
        ports:
        - containerPort: 9113
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen 80;
        location / {
          root /usr/share/nginx/html;
          index index.html index.htm;
        }
        location /stub_status {
          stub_status on;
          allow 127.0.0.1;
          deny all;
        }
      }
    }

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  annotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: '9113'
spec:
  selector:
    app: nginx
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
  - name: metrics
    protocol: TCP
    port: 9113
    targetPort: 9113
  type: ClusterIP
~~~
Здесь важно обратить внимения на раздел annotations, где мы разрешаем мониторинг через Prometheus и указываем порт, на котором он будет развёрнут. А также на раздел, где мы импортируем образ nginx/nginx-prometheus-exporter. (На самом деле подобрать правильные настройки для этого файла было сложнее всего и заняло очень много времени).
Далее, можно развернуть сервер с параметрами из файла одной командой 
~~~ps1
kubectl apply -f nginx-deployment.yaml
~~~
И последнее развёртывание - Графна. Её также устанавливаем через helm
~~~ps1
helm install grafana grafana/grafana
~~~
При установки появляется предложение запустить отдельную команду, чтобы получить пароль админа 
~~~ps1
kubectl get secret --namespace default grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
~~~
В PowerShell эта команда не заработала. Но можно заметить, что это составная команда, ошибка была именно в декодировании base64, а вот первая команда работала корректно. Поэтому я полчил пароль в кодировке base64 через первую часть команды. А дальше нашёл [онлайн-декодер](https://www.base64decode.org/) и получил там свой пароль. 
Теперь можно проверить, что все сервисы работают дейстивтельно работают в кластере и получить список подов: 
![image](https://github.com/user-attachments/assets/fa54b4fb-3d8e-4542-96de-54b9c7a645b3)
![image](https://github.com/user-attachments/assets/40b03613-2931-4e58-b48c-293184faf0a6)

И последний шаг на этом этапе свзязать порты подов и порты компьютера
~~~ps1
kubectl --namespace default port-forward grafana-5fc588d9df-w6t86 3000
kubectl port-forward service/prometheus-server 9090:80
kubectl port-forward svc/nginx-service 8081:80
~~~
Коману для графны я брал из подсказки при установки поэтому она указывает нэймспейс, хотя в нашем случае в этом необходимости нет, но главное что работает. Вообще эти команды не особо удобные, так как после запуска каждая полностью занимает окно с консолью, каждый раз приходиться новое окно открывать. Но опять же: зато работает.

### Настраиваем мониторинг ###
Теперь временно можно закрыть сташную и непонятную консоль и перейти в красивый интерфейс графны. Его можно найти на [localhost:3000](localhost:3000). Нас встречает окошко логина, туда надо ввести админский пароль, который я декодировал ранее

![image](https://github.com/user-attachments/assets/c54bdb72-937c-43ef-bf1b-ce97d8f07c46)

Хром даже запомнл этот пароль)
Дальше попадаем на главную страницу

![image](https://github.com/user-attachments/assets/3485cbc3-4dca-4fb0-b7db-1b1ad9e24467)

Прямо центру есть большая кнопка Add your first data source — нам туда! Тут нужно выбрать источник данных — удобно что Prometheus там самый первый в списке 

![image](https://github.com/user-attachments/assets/c174977d-14d9-4d0d-b282-5b5ca37924f8)

Дальше появляется окно с кучей настроеек, но я только добавил адрес Prometheus - [http://prometheus-server.default.svc.cluster.local:80](http://prometheus-server.default.svc.cluster.local:80)

![image](https://github.com/user-attachments/assets/8ca8905b-bfdb-48cc-a904-91eae55784cb)

Далее спускаемся в самый низ страницы и там можно проверить, что источник данных находиться — мне повезло и всё заработало с первого раза 

![image](https://github.com/user-attachments/assets/93eed022-9aa3-4dd3-86ce-748aa63fd985)

Теперь интерфейс предлагает создать создать первый дашборд. Вдохновлённый успехом, я пошёл сразу же создавать дашборд, думая что уже почти сделал лабу. Выбрал какую-то ранодмную метрику, понажимал на кнопки далее и сохранить и получил.....

![image](https://github.com/user-attachments/assets/75f56902-fa43-4eea-934f-d1cb044f353e)

График в виде горизонтальной черты на 0 — крайне информативный! Рандомной метрикой, кстати, оказалась agregator_discovery_aggregation_count_total (что это вообще?). Решил, что такого классного графика, наверное, будет недостаточно, поэтому дальше пришлось думать, какие метрики можно было бы собрать, чтобы они были не равные 0.
### Проверяем мониторинг ###
Полистав список метрик, я увидел название nginx_http_requests_total. Такую метрику будет легко проверить, поэтому я выбрал её. Эта метрика считает http запросы, значит, чтобы увидеть график, нужно эти запросы отправлять. Для этого я написал простой Powershell скрипт, который будет отправлять запросы на адрес nginx сервера с интервалом в 1 секунду 
~~~ps1
while ($true) {
    try {
        Invoke-WebRequest -Uri "http://localhost:8081" -UseBasicParsing | Out-Null
        Write-Output "Request sent successfully."
    } catch {
        Write-Output "Request failed: $_"
    }
    Start-Sleep -Seconds 1
}
~~~
Заработало не сразу, но зато после всех правок в консоли появились заветные сообщения Request sent successfully, появляющиеся с нужными инетрвалами. Теперь можно составить дашборд с выбранной метрикой и посмотреть, что там отображается

![image](https://github.com/user-attachments/assets/3de13300-e219-4889-a470-c3923d394477)

Сначала я отправлял запросы с интервалом 0,1 секунду и график соответсвено рос быстро. Затем интервал увеличелся до 1 секунду, угол график соответсвенно стал меньше. И, наконец, когда я совсем перестал слать запросы, график стал горизонтальным.

## Звёздочка ##

### Предисловие ###
Эту лабу я делал очень долго и получиилась она у меня далеко не сразу......

В какой-то момент я перебирал очень разные настройи из самых разных источников, большинство не дали результата (или вообще всё сломали). Этот отчёт - моя попытка востановить те шаги, которые привели меня к успеху. Возможно часть шагов в этой работе излишне, а возможно, наоборот, я забыл про какой-то фикс найденный на 5 странице гугла, который в моменте мне не помог, но по итогу оказался крайне важен для итогового результата, а я уже забыл про него и в отчёте ничего не написал.

В любом случае постараюсь при написании отчёта ещё раз уложить в голове весь проделанный путь.....

### Что было в начале ###
Итак, вспомним на чём я остановился в прошлой работе (которая 5 без звёздочки). В кубере у нас есть несколько отдельных сервисов: Nginx, Grafana, Prometheus. Полный список можно посмотреть с помощью команды
~~~ps1
kubectl get services
~~~
Ничего нового, по сравнению со скрином из отчёта по прошлой лабе, там не появилось. Но! Теперь внимание привлекает сервис, который в прошлой раз мне не понабился, а теперь звучит как раз то что нужно - prometheus-alertmanager. Неужели для этой лабы даже ничего устанавливать не придётся? Этот пока ещё непонятный мне alertmanager уже занимал порт 9093. Интересно узнать что там! 

Чтобы проверить, что происходит на этом порту, его нужно связать также как я это делал в прошлой лабе
```ps1
kubectl port-forward service/prometheus-alertmanager 9093
```
Теперь на [локалхосте](http://localhost:9093/#/alerts) меня встретил минималистичный интерфейс алёртменеджера. Там можно было посмотреть на список алёртов (пустой на тот момент). Но больше никаких полезных функций или настроек, я там для себя не нашёл 

![image](https://github.com/user-attachments/assets/e895adb7-4bca-48ae-aa0b-575baa3b74e7)

Ещё один интерфейс, который стоило проверить, это интерфейс самого прометеуса. К его использованию всё было готово ещё в прошлой лабе, но в тот раз он мне не понадобился. Так что полноценно осваивать его пришлось только сейчас 

![image](https://github.com/user-attachments/assets/bfdd292a-97ad-40a7-897c-88a6a563bd5f)

Тут пока тоже было мало чего полезного. Но позже, я открыл для себя вкладку targets, где можно было посмотреть, а что именно сейчас мониториться. 
### Настраиваем Alert Rule ###
На вкладке targets магией ctrl+F я нашёл целых 2 упоминания моего сервиса с nginx в разделе kubernetes-service-endpoints. 

![image](https://github.com/user-attachments/assets/a7dab928-3285-421e-9a6c-ba8393b17ceb)

Немного позже я выяснил, что эти 2 цели мониторнига соответствуют подам с nginx сервером. Можно бы было мониторить состаяние этих подов, и слать алёрты, если они перестают отвечать. 
Идея хорошая, и поэтому методом проб и ошибок я составил правило, которое будет выдавать алёрт, если поды не отвечают дольше 1 минуты 
```yaml
rules:
          - alert: NginxDownAlert
            expr: absent(up{job="kubernetes-service-endpoints", service="nginx-service"} == 1)
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Тревога!"
              description: "NGINX сервер не отвечает дольше 1 минуты!"
```

Очевидно самым сложным тут было подобрать правильное выражение в разделе expr. Проверять их можно было на главной странице прометеуса - это упростило работу. 

Это выражение я положил в файл alertmanager-config.yaml, а обновлять его можно было командой

```ps1
helm upgrade prometheus prometheus-community/prometheus -f alertmanager-config.yaml
```

А чтобы не беситься, почему мои изменения не отображаются, нужно после каждой такой команды перезапускать прометеус и алёртменеджер 
```ps1
kubectl rollout restart deployment prometheus-server
kubectl rollout restart statefulset prometheus-alertmanager
```

(Вообще вот эти 3 команды - лучшие друзья при выполении этой работы, их лучше всего было бы сразу поближе сохранить, потому что прогонять их нужно было ну очень часто)

И не забываем после этого заново настроить port-forwarding (выше уже были эти команды)

А чтобы составленное выражение было истиным, нужно было как-то отключить поды с nginx (и желательно чтобы потом можно было быстро всё вернуть). Тут на помощь пришла следующая команда 

```ps1
kubectl scale deployment nginx-deployment --replicas=0
```
Тут мы ограничиваем макимальное число подов, которое можнет занимать nginx-deployment. Поэтому при значении 0 все поды и соответственно kubernetes-service-endpoints пропадут. А вернуть их можно той же командой, поставив --replicas=2. Теперь я научился легко и быстро триггерить правило алёрта, составленное выше. 

Проверить это можно было в разделе Alerts в интерфейсе прометеуса 

![image](https://github.com/user-attachments/assets/50051d6a-39fc-4542-8e55-e5ae81b62c04)

Чуть позже я также выяснил, что этот же алёрт также теперь отображается и на главной на странице алёртменеджера. 

### Отправляем алёрт на почту ###

Отвлечёмся на минутку от командной стоки, yaml файлов и интерфейсов на локалхосте.

Мне показалось проще всего было бы слать алёрты себе на гугл почту. Для этого у gmail есть специальный сервер - [smtp.gmail.com:587](smtp.gmail.com:587). И даже настраивать ничего не надо. Почти. 

Дело в том, что использовать свой настоящий пароль для подобных задач небезопасно, поэтому предлагалось создать себе специальный app password, который алёртменеджер как раз сможет использовать для того чтобы слать письма. Делается это в 2 клика [тут](https://myaccount.google.com/apppasswords). Мне выдало 16 символов, между каждыми 4 были пробелы. В конфиге я их потом убрал, но вроде это было необязательно. 

А теперь обратно к настройки алёртменеджера 

Для отправки оповещений по почте в конфиге нужно было прописать 2 настройки - описать группу алёртов, от которых будут отправляться сообщения (тут указываются данные для входа в почту, и адрес получателя) и мэппинг всех входящих алёртов в эту группу. За эти настройки в yaml файле отвечают разделы receivers и route


```yaml
route:
      group_by: [Alertname]
      receiver: email-me

    receivers:
    - name: email-me
      email_configs:
      - to: gleb.shkoda@gmail.com
        from: gleb.shkoda@gmail.com
        smarthost: smtp.gmail.com:587
        auth_username: "gleb.shkoda@gmail.com"
        auth_identity: "gleb.shkoda@gmail.com"
        auth_password: "{На кусре по ИБ сказали, что нельзя заливать свои пароли на гитхаб :( }"
```
Если всё выполнено верно (то есть не как у меня по началу), то теперь в интерфейсе алёртменеджера все алёрты должны приходить в группу email-me, вместо default-group.

![image](https://github.com/user-attachments/assets/3f71a0bd-894f-46c1-8618-dbe9037d132b)

Здесь стоит поблагодорить вот этот [гайдик](https://www.robustperception.io/sending-email-with-the-alertmanager-via-gmail/), который очень помог правильно описать конфиг алёртменеджера.


### Результат ###

После всех этапов работы, итоговый файл alertmanager-config.yml выглядел вот так 
```yaml
alertmanager:
  config:
    route:
      group_by: [Alertname]
      receiver: email-me

    receivers:
    - name: email-me
      email_configs:
      - to: gleb.shkoda@gmail.com
        from: gleb.shkoda@gmail.com
        smarthost: smtp.gmail.com:587
        auth_username: "gleb.shkoda@gmail.com"
        auth_identity: "gleb.shkoda@gmail.com"
        auth_password: "{На кусре по ИБ сказали, что нельзя заливать свои пароли на гитхаб :( }"

serverFiles:
  alerting_rules.yml:
    groups:
      - name: nginx-alert
        rules:
          - alert: NginxDownAlert
            expr: absent(up{job="kubernetes-service-endpoints", service="nginx-service"} == 1)
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Тревога!"
              description: "NGINX сервер не отвечает дольше 1 минуты!"

```

И, если всё сделано правильно и звёзды сошлись - то на почту прийдёт вот такое письмо счастья 


![image](https://github.com/user-attachments/assets/61401ea8-e11a-4a66-b2a3-7cffc96173f9)


## Вывод ##

~~В результате выполнения данной работы на примере веб-сервера nginx, я научился поднимать сервисы в кубернетесе, настраивать мониторинг этих сервисов через prometheus, составлять визуализации метрик через grafana, и отправлять алёрты через alertmanager~~

Больше всего времени в этих 2 лабах отняли настройки yaml файлов и подбор настроек для них. Чаще всего приходилось брать какие-то готовые конфиги и долго-долго подгонять их под свои нуждны, каждый раз проверяя работает ли очередная версия файла. Но со временем правда стало проще - я понял как helm добавляет новые конфиги, когда я вызываю helm upgrade и из каких разделов состоят файлы конфигурации и что каждый из них делает. А когда в конце лабы под звёздочкой мне наконец пришёл на почту долгожданный алёрт - это вообще слёзы счастья!!
